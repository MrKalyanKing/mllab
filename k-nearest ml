import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the Iris dataset
iris = datasets.load_iris()
data = iris.data
labels = iris.target

# Print specific data points (Ensure valid indices)
for i in [0, 79, 99]:  # Removed 101 to avoid IndexError
    print(f"index: {i:3}, features: {data[i]}, label: {labels[i]}")

# Randomly shuffle indices and split into training and test sets
np.random.seed(42)
indices = np.random.permutation(len(data))
n_training_samples = 12  # Number of test samples

# Split the data into training and testing sets
learn_data = data[indices[:-n_training_samples]]
learn_labels = labels[indices[:-n_training_samples]]
test_data = data[indices[-n_training_samples:]]
test_labels = labels[indices[-n_training_samples:]]

# Print the first few samples of the training set
print("The first samples of our learn set:")
print(f"{'index':7s}{'data':20s}{'label':3s}")
for i in range(5):
    print(f"{i:4d} {learn_data[i]} {learn_labels[i]:3}")

# Print the first few samples of the test set
print("The first samples of our test set:")
print(f"{'index':7s}{'data':20s}{'label':3s}")
for i in range(5):
    print(f"{i:4d} {test_data[i]} {test_labels[i]:3}")

# Visualize the data in 3D
colours = ("r", "b", "y")
X = [[[], [], []] for _ in range(3)]  # Create empty lists for each class

# Fill the lists with data for each class
for i in range(len(learn_data)):
    class_label = learn_labels[i]
    X[class_label][0].append(learn_data[i][0])
    X[class_label][1].append(learn_data[i][1])
    X[class_label][2].append(sum(learn_data[i][2:]))

# Plot the 3D scatter plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
for iclass in range(3):
    ax.scatter(X[iclass][0], X[iclass][1], X[iclass][2], c=colours[iclass], label=f"Class {iclass}")

ax.set_xlabel("Feature 1")
ax.set_ylabel("Feature 2")
ax.set_zlabel("Feature 3+4 sum")
ax.legend()
plt.show()

# Calculate Euclidean distance
def distance(instance1, instance2):
    """ Calculates the Euclidean distance between two instances """
    return np.linalg.norm(instance1 - instance2)

# Find the k nearest neighbors
def get_neighbors(training_set, labels, test_instance, k, distance):
    """Get neighbors calculates a list of the k nearest neighbors of an instance"""
    distances = [(training_set[i], distance(test_instance, training_set[i]), labels[i]) 
                 for i in range(len(training_set))]
    
    distances.sort(key=lambda x: x[1])  # Sort by distance
    return distances[:k]  # Select top k neighbors

# Test the k-NN function for the test set
for i in range(5):  # Iterating over the first 5 test samples
    neighbors = get_neighbors(learn_data, learn_labels, test_data[i], 3, distance)
    
    print(f"Index: {i}")
    print(f"Testset Data: {test_data[i]}")
    print(f"Testset Label: {test_labels[i]}")
    print("Neighbors:")
    for neighbor in neighbors:
        print(f"  Features: {neighbor[0]}, Distance: {neighbor[1]:.2f}, Label: {neighbor[2]}")
    print()
